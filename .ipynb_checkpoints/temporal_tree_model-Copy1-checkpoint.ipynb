{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class treeEncoder(nn.Module):\n",
    "    def __init__(self,inp_dim,hid_dim,cuda):\n",
    "        super(treeEncoder,self).__init__()\n",
    "        self.inp_dim = inp_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.cudaAvailable = cuda\n",
    "        \n",
    "        self.ix = nn.Linear(self.inp_dim,self.hid_dim)\n",
    "        self.ih = nn.Linear(self.hid_dim,self.hid_dim)\n",
    "        \n",
    "        self.ux = nn.Linear(self.inp_dim,self.hid_dim)\n",
    "        self.uh = nn.Linear(self.hid_dim,self.hid_dim)\n",
    "        \n",
    "        self.ox = nn.Linear(self.inp_dim,self.hid_dim)\n",
    "        self.oh = nn.Linear(self.hid_dim,self.hid_dim)\n",
    "        \n",
    "        self.fx = nn.Linear(self.inp_dim,self.hid_dim)\n",
    "        self.fh = nn.Linear(self.hid_dim,self.hid_dim)\n",
    "        \n",
    "    def getParameters(self):\n",
    "        params = []\n",
    "        for m in [self.ix, self.ih, self.fx, self.fh, self.ox, self.oh, self.ux, self.uh]:\n",
    "            l = list(m.parameters())\n",
    "            params.extend(l)\n",
    "\n",
    "        one_dim = [p.view(p.numel()) for p in params]\n",
    "        params = F.torch.cat(one_dim)\n",
    "        return params\n",
    "        \n",
    "    def get_child_states(self, node):\n",
    "        if len(node.children) == 0:\n",
    "            child_c = Variable(torch.zeros(1,1,self.hid_dim))\n",
    "            child_h = Variable(torch.zeros(1,1,self.hid_dim))\n",
    "            \n",
    "            if self.cudaAvailable:\n",
    "                child_c = child_c.to('cuda:1')\n",
    "                child_h = child_h.to('cuda:1')\n",
    "            \n",
    "        else:\n",
    "            child_c = Variable(torch.Tensor(len(node.children),1,self.hid_dim))\n",
    "            child_h = Variable(torch.Tensor(len(node.children),1,self.hid_dim))\n",
    "            \n",
    "            if self.cudaAvailable:\n",
    "                child_c, child_h = child_c.to('cuda:1'), child_h.to('cuda:1')\n",
    "            \n",
    "            for child_num in range(len(node.children)):\n",
    "                child_c[child_num] = node.children[list(node.children)[child_num]].state[0]\n",
    "                child_h[child_num] = node.children[list(node.children)[child_num]].state[1]\n",
    "        \n",
    "        return child_c, child_h\n",
    "    \n",
    "    def node_forward(self,inputs, child_c, child_h):\n",
    "       \n",
    "        child_h_sum = F.torch.sum(torch.squeeze(child_h,1),0)\n",
    "        \n",
    "#         print(inputs.type())\n",
    "#         print(child_h_sum.type())\n",
    "        \n",
    "        i = F.sigmoid(self.ix(inputs)+self.ih(child_h_sum))\n",
    "        o = F.sigmoid(self.ox(inputs)+self.oh(child_h_sum))\n",
    "        u = F.tanh(self.ux(inputs)+self.uh(child_h_sum))\n",
    "\n",
    "        # add extra singleton dimension\n",
    "        fx = F.torch.unsqueeze(self.fx(inputs),1)\n",
    "        f = F.torch.cat([self.fh(child_hi)+fx for child_hi in child_h], 0)\n",
    "        f = F.sigmoid(f)\n",
    "\n",
    "#         f = F.torch.unsqueeze(f,1) # comment to fix dimension missmatch\n",
    "        fc = F.torch.squeeze(F.torch.mul(f,child_c),1)\n",
    "\n",
    "        c = F.torch.mul(i,u) + F.torch.sum(fc,0)\n",
    "        h = F.torch.mul(o, F.tanh(c))\n",
    "        \n",
    "        return child_c,child_h\n",
    "        return c, h\n",
    "    \n",
    "    def forward(self,node,userVects):        \n",
    "        for child_uid in node.children:\n",
    "            self.forward(node.children[child_uid],userVects)\n",
    "        \n",
    "        child_c,child_h = self.get_child_states(node)\n",
    "        \n",
    "        print(node.uid)\n",
    "        node.state = self.node_forward(userVects[node.uid],child_c,child_h)\n",
    "\n",
    "        return node.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct ChildSumTreeLSTM Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputModule(nn.Module):\n",
    "    def __init__(self, cuda, mem_dim, num_classes, device, dropout = False):\n",
    "        super(OutputModule, self).__init__()\n",
    "        self.cudaFlag = cuda\n",
    "        self.mem_dim = mem_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self.l1 = nn.Linear(self.mem_dim, self.num_classes)\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "        if self.cudaFlag:\n",
    "            self.l1 = self.l1.to(device)\n",
    "\n",
    "    def forward(self, vec, training = False):\n",
    "        if self.dropout:\n",
    "            out = self.logsoftmax(self.l1(F.dropout(vec, training = training)))\n",
    "        else:\n",
    "            out = self.logsoftmax(self.l1(vec))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fixedTreeEncoder(nn.Module):\n",
    "    def __init__(self, cuda,in_dim, mem_dim,userVects,labels,labelMap,criterion,device):\n",
    "        super(fixedTreeEncoder, self).__init__()\n",
    "        self.cudaFlag = cuda\n",
    "        self.in_dim = in_dim\n",
    "        self.mem_dim = mem_dim\n",
    "        self.device = device\n",
    "        self.labels = labels\n",
    "        self.labelMap = labelMap\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.ix = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.ih = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "\n",
    "        self.fx = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.fh = nn.Linear(self.mem_dim, self.mem_dim)\n",
    "\n",
    "        self.ux = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.uh = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "\n",
    "        self.ox = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.oh = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "        \n",
    "        self.userVects = userVects\n",
    "        self.outputModule = OutputModule(self.cudaFlag,mem_dim,4,self.device,dropout=False)\n",
    "    \n",
    "    def predict(self,node):\n",
    "        loss = Variable(torch.zeros(1))\n",
    "        if self.cudaFlag:\n",
    "            loss = loss.to(self.device)\n",
    "        \n",
    "        for i in range(node.num_children):\n",
    "            _, child_loss = self.forward(node.childrenList[i])\n",
    "            loss = loss + child_loss\n",
    "        child_c, child_h = self.getChildStates(node)\n",
    "        node.state = self.nodeForward(self.userVects[node.uid],child_c,child_h)\n",
    "        \n",
    "        output = self.outputModule.forward(node.state[1], False)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def forward(self,node):\n",
    "        loss = Variable(torch.zeros(1))\n",
    "        if self.cudaFlag:\n",
    "            loss = loss.to(self.device)\n",
    "        \n",
    "        for i in range(node.num_children):\n",
    "            _, child_loss = self.forward(node.childrenList[i])\n",
    "            loss = loss + child_loss\n",
    "        child_c, child_h = self.getChildStates(node)\n",
    "        node.state = self.nodeForward(self.userVects[node.uid],child_c,child_h)\n",
    "        \n",
    "        output = self.outputModule.forward(node.state[1], True)\n",
    "        node.output = output\n",
    "            \n",
    "        label = Variable(torch.tensor(self.labelMap[node.label]))\n",
    "                \n",
    "        if self.cudaFlag:\n",
    "            label = label.to(self.device)\n",
    "            \n",
    "        loss = loss + self.criterion(output.reshape(-1,4), label.reshape(-1))\n",
    "        \n",
    "        return node.state, loss\n",
    "        \n",
    "    def nodeForward(self,x,child_c,child_h):\n",
    "        # h^~_j = sum of child hidden states\n",
    "        child_h_sum = torch.sum(child_h,0)\n",
    "\n",
    "        i = torch.sigmoid(self.ix(x) + self.ih(child_h_sum))\n",
    "        o = torch.sigmoid(self.ox(x)+self.oh(child_h_sum))\n",
    "        u = torch.tanh(self.ux(x)+self.uh(child_h_sum))\n",
    "        \n",
    "        fx = self.fx(x)\n",
    "        f = torch.cat([self.fh(child_hi)+fx for child_hi in child_h], 0)\n",
    "        fc = torch.sigmoid(f)\n",
    "        \n",
    "        c = i*u + torch.sum(fc,0)\n",
    "        h = o*torch.tanh(c)\n",
    "        \n",
    "        return c,h\n",
    "    \n",
    "    def getChildStates(self,node):\n",
    "        if node.num_children==0:\n",
    "            child_c = Variable(torch.zeros(1,self.mem_dim))\n",
    "            child_h = Variable(torch.zeros(1,self.mem_dim))\n",
    "            if self.cudaFlag:\n",
    "                child_c, child_h = child_c.to(self.device), child_h.to(self.device)\n",
    "        \n",
    "        else:\n",
    "            child_c = Variable(torch.Tensor(node.num_children,self.mem_dim))\n",
    "            child_h = Variable(torch.Tensor(node.num_children,self.mem_dim))\n",
    "            if self.cudaFlag:\n",
    "                child_c, child_h = child_c.to(self.device), child_h.to(self.device)\n",
    "            \n",
    "            for idx in range(node.num_children):\n",
    "                child_c[idx] = node.childrenList[idx].state[0]\n",
    "                child_h[idx] = node.childrenList[idx].state[1]\n",
    "        return child_c, child_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChildSumTreeLSTM(nn.Module):\n",
    "    def __init__(self, cuda, in_dim, mem_dim):\n",
    "        super(ChildSumTreeLSTM, self).__init__()\n",
    "        self.cudaFlag = cuda\n",
    "        self.in_dim = in_dim\n",
    "        self.mem_dim = mem_dim\n",
    "\n",
    "        self.ix = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.ih = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "\n",
    "        self.fh = nn.Linear(self.mem_dim, self.mem_dim)\n",
    "        self.fx = nn.Linear(self.in_dim,self.mem_dim)\n",
    "\n",
    "        self.ux = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.uh = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "\n",
    "        self.ox = nn.Linear(self.in_dim,self.mem_dim)\n",
    "        self.oh = nn.Linear(self.mem_dim,self.mem_dim)\n",
    "\n",
    "\n",
    "    def getParameters(self):\n",
    "        \"\"\"\n",
    "        Get flatParameters\n",
    "        note that getParameters and parameters is not equal in this case\n",
    "        getParameters do not get parameters of output module\n",
    "        :return: 1d tensor\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for m in [self.ix, self.ih, self.fx, self.fh, self.ox, self.oh, self.ux, self.uh]:\n",
    "            # we do not get param of output module\n",
    "            l = list(m.parameters())\n",
    "            params.extend(l)\n",
    "\n",
    "        one_dim = [p.view(p.numel()) for p in params]\n",
    "        params = F.torch.cat(one_dim)\n",
    "        return params\n",
    "\n",
    "\n",
    "    def node_forward(self, inputs, child_c, child_h):\n",
    "        \"\"\"\n",
    "        :param inputs: (1, 300)\n",
    "        :param child_c: (num_children, 1, mem_dim)\n",
    "        :param child_h: (num_children, 1, mem_dim)\n",
    "        :return: (tuple)\n",
    "        c: (1, mem_dim)\n",
    "        h: (1, mem_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        child_h_sum = F.torch.sum(torch.squeeze(child_h,1),0)\n",
    "\n",
    "        i = F.sigmoid(self.ix(inputs)+self.ih(child_h_sum))\n",
    "        o = F.sigmoid(self.ox(inputs)+self.oh(child_h_sum))\n",
    "        u = F.tanh(self.ux(inputs)+self.uh(child_h_sum))\n",
    "\n",
    "        # add extra singleton dimension\n",
    "        fx = F.torch.unsqueeze(self.fx(inputs),1)\n",
    "        f = F.torch.cat([self.fh(child_hi)+fx for child_hi in child_h], 0)\n",
    "        f = F.sigmoid(f)\n",
    "\n",
    "        # f = F.torch.unsqueeze(f,1) # comment to fix dimension missmatch\n",
    "        fc = F.torch.squeeze(F.torch.mul(f,child_c),1)\n",
    "\n",
    "        c = F.torch.mul(i,u) + F.torch.sum(fc,0)\n",
    "        h = F.torch.mul(o, F.tanh(c))\n",
    "\n",
    "        return c, h\n",
    "\n",
    "    def forward(self, tree, embs, training = False):\n",
    "        \"\"\"\n",
    "        Child sum tree LSTM forward function\n",
    "        :param tree:\n",
    "        :param embs: (sentence_length, 1, 300)\n",
    "        :param training:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # add singleton dimension for future call to node_forward\n",
    "        # embs = F.torch.unsqueeze(self.emb(inputs),1)\n",
    "\n",
    "        loss = Variable(torch.zeros(1)) # init zero loss\n",
    "        if self.cudaFlag:\n",
    "            loss = loss.cuda()\n",
    "\n",
    "        print('number of children to go through: ',tree.num_children)\n",
    "        for idx in range(tree.num_children):\n",
    "            print('current child number: ', idx,' for node: ',tree.uid)\n",
    "            _, child_loss = self.forward(tree.childrenList[idx], embs, training)\n",
    "            loss = loss + child_loss\n",
    "        child_c, child_h = self.get_child_states(tree)\n",
    "        tree.state = self.node_forward(userVects[tree.uid], child_c, child_h)\n",
    "        \n",
    "        return tree.state, loss\n",
    "\n",
    "    def get_child_states(self, tree):\n",
    "        \"\"\"\n",
    "        Get c and h of all children\n",
    "        :param tree:\n",
    "        :return: (tuple)\n",
    "        child_c: (num_children, 1, mem_dim)\n",
    "        child_h: (num_children, 1, mem_dim)\n",
    "        \"\"\"\n",
    "        # add extra singleton dimension in middle...\n",
    "        # because pytorch needs mini batches... :sad:\n",
    "        if tree.num_children==0:\n",
    "            child_c = Variable(torch.zeros(1,1,self.mem_dim))\n",
    "            child_h = Variable(torch.zeros(1,1,self.mem_dim))\n",
    "            if self.cudaFlag:\n",
    "                child_c, child_h = child_c.cuda(), child_h.cuda()\n",
    "        else:\n",
    "            child_c = Variable(torch.Tensor(tree.num_children,1,self.mem_dim))\n",
    "            child_h = Variable(torch.Tensor(tree.num_children,1,self.mem_dim))\n",
    "            if self.cudaFlag:\n",
    "                child_c, child_h = child_c.cuda(), child_h.cuda()\n",
    "            for idx in range(tree.num_children):\n",
    "                child_c[idx] = tree.childrenList[idx].state[0]\n",
    "                child_h[idx] = tree.childrenList[idx].state[1]\n",
    "        return child_c, child_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "fakenews"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
